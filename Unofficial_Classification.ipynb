{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5869bcd3-45c1-4cfb-9b86-3338e8a9b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import mixture, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.covariance import empirical_covariance\n",
    "\n",
    "\n",
    "# import scipy\n",
    "import json \n",
    "import random\n",
    "import numpy as np\n",
    "import itertools \n",
    "import sys\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "# from scipy import linalg\n",
    "\n",
    "# import word embeddings models \n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Remove all the un-necessary warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "path = 'CONAN-master/Multitarget-CONAN/'\n",
    "file = 'Multitarget-CONAN.json'\n",
    "file_p = path + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa96842-0636-4ecd-81be-85d02d41f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install _ # use ! to call commandline commands \n",
    "\n",
    "# one hot encode target categories \n",
    "pd.get_dummies(min_tar_orig, columns=[\"TARGET\"], prefix=\"target\").head()\n",
    "min_tar_dum = pd.get_dummies(min_tar_orig, columns=[\"TARGET\"], prefix=\"target\")\n",
    "print(min_tar_dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779e53c-b940-485f-bdd3-87e43114b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = mt_df_T\n",
    "# test_df[\"TARGET\"] = min_tar\n",
    "# print(test_df)\n",
    "\n",
    "# select K best of top 8 features probably \n",
    "\n",
    "# Likely candidates for feature reduction \n",
    "# mt_df['TARGET']=np.where(mt_df['TARGET'] =='POC', 'FOREIGNER', mt_df['TARGET'])\n",
    "# mt_df['TARGET']=np.where(mt_df['TARGET'] =='MUSLIM', 'FOREIGNER', mt_df['TARGET'])\n",
    "# mt_df['TARGET']=np.where(mt_df['TARGET'] =='MIGRANTS', 'FOREIGNER', mt_df['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b72d242-2de7-4603-8aa0-660ddd2e1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guidance on usage and pre-processing for BERT:\n",
    "# https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "\n",
    "t = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\", \n",
    "                                  output_hidden_states = True,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3bf8f9-7463-40e7-b519-035d272442b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_tok = [] # pd.DataFrame(dtype = 'object') \n",
    "hs_feat = [] # pd.DataFrame(dtype = 'object') \n",
    "target_word_embeddings = []\n",
    "# Tweet 1 \n",
    "embed1 = \"Maybe the UN could talk to those asian and african nations responsible for 90%+ of the pollution in the oceans' instead of insisting on this bullshit about climate change.\"\n",
    "\n",
    "ind = 0 \n",
    "for doc in hs_orig: \n",
    "    # doc = \"[CLS] \" + doc + \" [SEP]\"\n",
    "    t1 = t.tokenize(doc)\n",
    "    ind_t1 = t.convert_tokens_to_ids(t1)\n",
    "    seg_ids = [1]*len(ind_t1)\n",
    "#     print(t1)\n",
    "    hs_tok.append(t1) \n",
    "    \n",
    "    # encode document text as quantitative features for training \n",
    "    t2 = t.encode(t1)\n",
    "    #     print(t2)\n",
    "    hs_feat.append(t2)\n",
    "    \n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([ind_t1])\n",
    "    segments_tensors = torch.tensor([seg_ids])\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "# print(list_token_embeddings)\n",
    "    # Find the position 'hate' in list of tokens\n",
    "    # Get the embedding for hate\n",
    "    if embed1 in hs_tok[ind]: \n",
    "        # print(\"len hs_tok[ind]:\", len(hs_tok[ind]))\n",
    "        word_index = (hs_tok[ind]).index(embed1)\n",
    "        print(\"word_index:\", word_index)\n",
    "        # print(\"hs_tok[ind]:\", hs_tok[ind]) \n",
    "        # print(\"list_token_embeddings:\", list_token_embeddings)\n",
    "        # print(\"len list_token_embeddings:\", len(list_token_embeddings))\n",
    "        # print(\"list_token_embeddings[word_index]:\", list_token_embeddings[word_index])\n",
    "        word_embedding = list_token_embeddings[word_index]\n",
    "        target_word_embeddings.append(word_embedding)\n",
    "    else:\n",
    "        ind = ind \n",
    "        # target_word_embeddings.append(0.0)\n",
    "        # target_word_embeddings.append(\"\")\n",
    "\n",
    "    ind += 1 \n",
    "\n",
    "# print(hs_tok)\n",
    "# print(hs_feat)\n",
    "# print(pd.DataFrame(hs_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f8294-1a83-45de-ab95-39322ee4fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculating the distance between the\n",
    "# embeddings of '' in all the\n",
    "# given contexts of the word\n",
    "\n",
    "list_of_distances = []\n",
    "for text1, embed1 in zip(hs_orig, target_word_embeddings):\n",
    "    for text2, embed2 in zip(hs_orig, target_word_embeddings):\n",
    "        cos_dist = 1 - cosine(embed1, embed2)\n",
    "        list_of_distances.append([text1, text2, cos_dist])\n",
    "\n",
    "distances_df = pd.DataFrame(list_of_distances, columns=['text1', 'text2', 'distance'])\n",
    "display(distances_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
