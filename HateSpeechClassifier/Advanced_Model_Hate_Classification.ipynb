{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv # for excel too \n",
    "import json \n",
    "import random\n",
    "import numpy as np\n",
    "import itertools \n",
    "import matplotlib as mpl\n",
    "import sys\n",
    "import os\n",
    "import copy \n",
    "import pickle \n",
    "import scikitplot as skplt # from https://github.com/reiinakano/scikit-plot\n",
    "from scipy import sparse\n",
    "# import scipy\n",
    "\n",
    "# for natural language processing \n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import mixture, metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier # similar to Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix, multilabel_confusion_matrix, \\\n",
    "    classification_report, accuracy_score, plot_roc_curve, RocCurveDisplay\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_classif\n",
    "from sklearn.covariance import empirical_covariance\n",
    "\n",
    "# import word embeddings models \n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# LSTM and CNN *** sharfard paper \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Use to reload changed modules\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# How to debug\n",
    "'''After runtime error, open new cell and type %debug and run the cell. \n",
    "Opens a command line where you can test your code and inspect all variables right up to the line \n",
    "that threw the error.\n",
    "Type “n” and hit Enter to run the next line of code \n",
    "(The → arrow shows you the current position). Use “c” to continue until the next breakpoint. \n",
    "“q” quits the debugger and code execution.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python HateSpeechClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV] END ......lr=0.05, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END ......lr=0.05, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END ......lr=0.05, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END ......lr=0.05, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END ......lr=0.05, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END ......lr=0.09, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END ......lr=0.09, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END ......lr=0.09, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END ......lr=0.09, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END ......lr=0.09, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END .......lr=0.1, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END .......lr=0.1, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END .......lr=0.1, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END .......lr=0.1, max_epochs=20, module__num_units=128; total time=   0.0s\n",
      "[CV] END .......lr=0.1, max_epochs=20, module__num_units=128; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\classifier.py\", line 141, in fit\n",
      "    return super(NeuralNetClassifier, self).fit(X, y, **fit_params)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\", line 1215, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\", line 1174, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\", line 1088, in fit_loop\n",
      "    step_fn=self.train_step, **fit_params)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\", line 1122, in run_single_epoch\n",
      "    step = step_fn(batch, **fit_params)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\", line 1007, in train_step\n",
      "    self._step_optimizer(step_fn)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\", line 963, in _step_optimizer\n",
      "    optimizer.step(step_fn)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\torch\\optim\\optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\torch\\optim\\sgd.py\", line 120, in step\n",
      "    loss = closure()\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\", line 997, in step_fn\n",
      "    step = self.train_step_single(batch, **fit_params)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\", line 896, in train_step_single\n",
      "    y_pred = self.infer(Xi, **fit_params)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\", line 1359, in infer\n",
      "    return self.module_(x, **fit_params)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\trice\\AppData\\Local\\Temp\\ipykernel_24564\\1718820652.py\", line 62, in forward\n",
      "    output, hidden = self.lstm(emb, hidden)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\", line 752, in forward\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\trice\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [nan nan nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24564\\1718820652.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m     }\n\u001b[0;32m    104\u001b[0m     \u001b[0mgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#refit = False, cv = 5, scoring = 'accuracy', verbose = 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"best score: {:.3f}, best params: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\classifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# this is actually a pylint bug:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;31m# https://github.com/PyCQA/pylint/issues/1085\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNeuralNetClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[0;32m   1172\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'on_train_begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1174\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1175\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[0;32m   1086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m             self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n\u001b[1;32m-> 1088\u001b[1;33m                                   step_fn=self.train_step, **fit_params)\n\u001b[0m\u001b[0;32m   1089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m             self.run_single_epoch(dataset_valid, training=False, prefix=\"valid\",\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\u001b[0m in \u001b[0;36mrun_single_epoch\u001b[1;34m(self, dataset, training, prefix, step_fn, **fit_params)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_batch_begin\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             batch_size = (get_len(batch[0]) if isinstance(batch, (tuple, list))\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, batch, **fit_params)\u001b[0m\n\u001b[0;32m   1005\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstep_accumulator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\u001b[0m in \u001b[0;36m_step_optimizer\u001b[1;34m(self, step_fn)\u001b[0m\n\u001b[0;32m    961\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\u001b[0m in \u001b[0;36mstep_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zero_grad_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 997\u001b[1;33m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    998\u001b[0m             \u001b[0mstep_accumulator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\u001b[0m in \u001b[0;36mtrain_step_single\u001b[1;34m(self, batch, **fit_params)\u001b[0m\n\u001b[0;32m    894\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[0mXi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpack_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 896\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    897\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\skorch\\net.py\u001b[0m in \u001b[0;36minfer\u001b[1;34m(self, x, **fit_params)\u001b[0m\n\u001b[0;32m   1357\u001b[0m             \u001b[0mx_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_merge_x_and_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_predict_nonlinearity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24564\\1718820652.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mdecoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Twitter_Counter_Narrative\\tcn_venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    750\u001b[0m                         msg = (\"For unbatched 2-D input, hx and cx should \"\n\u001b[0;32m    751\u001b[0m                                f\"also be 2-D but got ({hx[0].dim()}-D, {hx[1].dim()}-D) tensors\")\n\u001b[1;32m--> 752\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m                     \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "from Embedding import Embedding, utils_preprocess_text\n",
    "from Model import Model, MODELS_PATH \n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable \n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "# # take from quickstart: https://skorch.readthedocs.io/en/stable/user/quickstart.html\n",
    "X_n, y_n = make_classification(1000, 20, n_informative = 10, random_state=0)\n",
    "# print(X_n)\n",
    "# print(X_n.shape[0], X_n.shape[1])\n",
    "# print(y_n) \n",
    "# print(y_n.shape[0])\n",
    "X_n = X_n.astype(np.float32)\n",
    "y_n = y_n.astype(np.int64)\n",
    "vocab_size = X_n.shape[1] # len(X_n) \n",
    "\n",
    "class LSTM(nn.Module):  \n",
    "    def __init__(self, dim = 128, num_units = 256, hidden = 128, dropout = 0.2, batch = 1, nonlin = F.relu): # nn.ReLU()): # 300 \n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_tokens = num_units # could be X.shape[1] \n",
    "        self.lstm_size = dim # X.shape[1] # 128 # input\n",
    "        self.hidden_size = self.lstm_size # 256 # hidden \n",
    "        self.num_layers = 3 # layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = self.lstm_size,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            dropout = dropout,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.encoder = nn.Embedding(self.num_tokens, self.lstm_size) # num_token, num_input \n",
    "        self.fc = nn.Linear(self.hidden_size, self.num_tokens) # num_hidden, num_token (vocab)\n",
    "        \n",
    "        self.init_weights()\n",
    "        self.batch_size = batch\n",
    "        \n",
    "        weight = next(self.parameters())\n",
    "        self.hidden = (weight.new_zeros(self.num_layers, self.batch_size, self.hidden_size),\n",
    "                       weight.new_zeros(self.num_layers, self.batch_size, self.hidden_size)) \n",
    "        \n",
    "        # self.dense0 = nn.Linear(self.lstm_size, num_units) # 20, num_units)\n",
    "        # self.nonlin = nonlin\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        # self.dense1 = nn.Linear(num_units, self.lstm_size)\n",
    "        # self.output = nn.Linear(self.lstm_size, 2)\n",
    "        # self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, X): # hidden \n",
    "        # X = (self.nonlin(self.dense0(X)))\n",
    "        # X = (self.dropout(X))\n",
    "        # X = (F.relu(self.dense1(X)))\n",
    "        # X = (F.softmax(self.output(X), dim = -1))\n",
    "        # print(X)\n",
    "        # return X\n",
    "        emb = self.dropout(X) \n",
    "        hidden = self.hidden\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        output = self.dropout(output)\n",
    "        decoded = self.fc(output)\n",
    "        self.hidden = hidden \n",
    "        return decoded, hidden\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    # def init_hidden(self, bsz):\n",
    "    #     weight = next(self.parameters())\n",
    "    #     hidden = (weight.new_zeros(self.num_layers, bsz, self.hidden_size),\n",
    "    #               weight.new_zeros(self.num_layers, bsz, self.hidden_size))\n",
    "    #     return (hidden)\n",
    "\n",
    "# # Old tutorial \n",
    "# # https://github.com/skorch-dev/skorch/blob/master/examples/rnn_classifer/RNN_sentiment_classification.ipynbclass LSTM(nn.Module):\n",
    "model = NeuralNetClassifier(\n",
    "        LSTM(),\n",
    "        module__dim = vocab_size, \n",
    "        module__hidden = vocab_size, \n",
    "        # module__num_units = 128,\n",
    "        # module__dropout = 0.2,\n",
    "        max_epochs = 10,\n",
    "        lr = 0.01,\n",
    "        # Shuffle training data on each epoch\n",
    "        iterator_train__shuffle = True,\n",
    "        # device=('cuda' if USE_CUDA else 'cpu'),\n",
    "    )\n",
    "# batch_size = 1\n",
    "# model.init_hidden(batch_size) \n",
    "    \n",
    "if cross_validate: \n",
    "    model.set_params(module__hidden = vocab_size, verbose = 0)\n",
    "    params = {\n",
    "        'lr': [0.05, 0.09, 0.1],\n",
    "        'max_epochs': [20], # [10, 20]\n",
    "        'module__num_units': [128] # , 256, 300]\n",
    "    }\n",
    "    gs = GridSearchCV(model, params, cv = 5, verbose = 2) #refit = False, cv = 5, scoring = 'accuracy', verbose = 2)\n",
    "    gs.fit(X_n, y_n)\n",
    "    print(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REPORTS_PATH = 'Reports/'\n",
    "SEED = 42 # Mo - maybe we should  try and use the same seed number throughout the code? - was 100\n",
    "TARGET_TYPES = ['Disabled', 'Jews', 'LGBT+', 'Migrants', 'Muslims', 'POC', 'Women', 'Other/Mixed', 'None']\n",
    "\n",
    "processed_data = pd.read_csv('Data/processed_combined_data.csv', index_col = False) # ignore index column         \n",
    "    # print(processed_data)\n",
    "texts = processed_data['HATE_SPEECH']\n",
    "class_labels = processed_data['CLASS']\n",
    "\n",
    "# print(texts, class_labels)\n",
    "debug = True \n",
    "train_test = True\n",
    "embedding_types = ['tfidf', 'word2vec', 'doc2vec', 'glove', 'bert']\n",
    "model_types = ['LR', 'RF', 'NB', 'DT', 'XGB', 'SVC', 'LSTM']\n",
    "model_type = \"LSTM\" \n",
    "\n",
    "with_stopwords = False\n",
    "weighting_type = 'equal'\n",
    "dimensions = 300\n",
    "scoring = ['accuracy', 'balanced_accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'] \n",
    "\n",
    "# Add scoring for hyperparameter tuning \n",
    "# Final cross validation dictionary/dataframe comparison \n",
    "model_dfs = [] #  {} \n",
    "\n",
    "# remove stopwords if necessary\n",
    "if not with_stopwords:\n",
    "    stop_words = stopwords.words('english')\n",
    "    texts = [t for t in texts if not t in stop_words]\n",
    "\n",
    "if train_test: \n",
    "    X_train_text, X_test_text, y_train, y_test = train_test_split(texts, class_labels, \n",
    "                                                                  random_state = SEED, test_size = 0.2)\n",
    "else: \n",
    "    X_train_text = texts # vectorize texts for 100% data for train\n",
    "    y_train = class_labels # class_labels for 100% data for train\n",
    "\n",
    "for embedding_type in embedding_types:\n",
    "    if debug:\n",
    "        print('----------------------------------------')\n",
    "        print(embedding_type)\n",
    "\n",
    "    # get embeddings for input text\n",
    "    embedding = Embedding(embedding_type, \n",
    "                          with_stopwords = with_stopwords, \n",
    "                          weighting = weighting_type,\n",
    "                          dimensions = dimensions)\n",
    "\n",
    "    embedding_filename = embedding.get_filename()\n",
    "\n",
    "    # vectorize train and test set\n",
    "    X = embedding.vectorize(X_train_text, load_train = True) \n",
    "    y = y_train  \n",
    "    # print(X.shape[0])\n",
    "    # print(y.shape[0]) \n",
    "    \n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int64)\n",
    "    vocab_size = X.shape[1]\n",
    "    \n",
    "    # print(X)\n",
    "    # print(X.shape)\n",
    "    # print(y) \n",
    "    # print(y.shape) \n",
    "    # X = np.reshape(X, (X.shape[1], X.shape[0]))\n",
    "    # print(X)\n",
    "    # print(X.shape) \n",
    "\n",
    "    # might need to save tfidf vectorizer and matrix for later use\n",
    "    # embedding.save(train_test_split=True)\n",
    "    if train_test: \n",
    "        X_test = embedding.vectorize(X_test_text, unseen = True, load_test = True)\n",
    "        # embedding.save(train_test_split=True, save_test=True)\n",
    "\n",
    "    model_params = {\n",
    "                    'random_state' : SEED \n",
    "                   }\n",
    "    # model_num = 0\n",
    "    debug = debug  \n",
    "    output = True \n",
    "    save = True # False \n",
    "    num = True\n",
    "    cross_validate = True # whether to fine tune the hyper parameters of the model with a cross validator \n",
    "    outfile = \"\"     \n",
    "    model_name = model_type + \"_\" + embedding_filename \n",
    "    if debug:\n",
    "        print('----------------------------------------')\n",
    "        print(model_type)\n",
    "        print('----------------------------------------')\n",
    "        print(\"Filename \")\n",
    "        print(\"\", model_name) \n",
    "        \n",
    "    model = NeuralNetClassifier(\n",
    "        LSTM(),\n",
    "        module__dim = vocab_size, \n",
    "        # module__num_units = 128,\n",
    "        # module__dropout = 0.2,\n",
    "        max_epochs = 10,\n",
    "        lr = 0.01,\n",
    "        # Shuffle training data on each epoch\n",
    "        iterator_train__shuffle = True,\n",
    "    )\n",
    "    \n",
    "    if cross_validate: \n",
    "        model.set_params(verbose = 0)\n",
    "        params_grid = {\n",
    "            'lr': [0.01, 0.02, 0.05],\n",
    "            'max_epochs': [20], # [10, 20]\n",
    "            'module__num_units': [128] # , 256, 300]\n",
    "        }\n",
    "        gs = GridSearchCV(model, params_grid, cv = 5, verbose = 2) #refit = False, cv = 5, scoring = 'accuracy', verbose = 2)\n",
    "        # print(y.dtype)\n",
    "        # y = y.astype(np.float64)\n",
    "        # print(y.dtype) \n",
    "        gs.fit(X, y)\n",
    "        print(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))\n",
    "        \n",
    "    # model.fit(X, y)\n",
    "    # y_proba = model.predict_proba(X)\n",
    "    # y_proba = gs.predict_proba(X) \n",
    "    # print(y_proba)\n",
    "    \n",
    "    # model = NeuralNet(model_type, X = X, y = y, debug = debug, \n",
    "    #               model_params = model_params, filename = model_name) #max_iter=2000) \n",
    "    \n",
    "    # LSTM(X, y, debug = debug) #max_iter=2000)\n",
    "#     mm = MinMaxScaler()\n",
    "#     ss = StandardScaler()\n",
    "#     X_ss = ss.fit_transform(X)\n",
    "#     y_mm = mm.fit_transform(y) \n",
    "    \n",
    "#     X_train = X_ss[:int(0.8 * len(X)), :]\n",
    "#     X_test = X_ss[int(0.8 * len(X)):, :]\n",
    "#     y_train = y_mm[:int(0.8 * len(y)), :] \n",
    "#     y_test = y_mm[int(0.8 * len(X)):, :]\n",
    "#     batches = [batch_size] * len(X) \n",
    "#     train(model)    \n",
    "    # model.fit(X, y) \n",
    "                \n",
    "    # y_preds = model.predict(X_test)\n",
    "    # print(pd.concat[y_test, y_preds]) \n",
    "#     cls_report = model.classification_report(y_test, y_preds, output = output, save = save, \n",
    "#                                              path = REPORTS_PATH) #, path = reports_path)\n",
    "#     acc_report = model.accuracy_score(y_test, y_preds, path = REPORTS_PATH)\n",
    "#     conf_m = model.confusion_matrix(y_test, y_preds, output = output, save = save, \n",
    "#                                     path = REPORTS_PATH) #, path = reports_path)\n",
    "\n",
    "#     model.heatmap(save = save, num = num, path = REPORTS_PATH) # path = reports_path)\n",
    "#     model.roc_curve(X_test, y_test, save = save, path = REPORTS_PATH) # path = reports_path)\n",
    "#     # model.report_results(output = output, save = save, path = REPORTS_PATH) # path = reports_path) \n",
    "#     model.save_model(MODELS_PATH)\n",
    "\n",
    "    # print(\"Cross Validation Evaluation: \") \n",
    "    # print(\"Cross validator: \", model.CV) \n",
    "    # print(\"Old estimator: \", model.untuned_model) \n",
    "    # print(\"Best estimator: \", model.model)\n",
    "    # print(\"Best parameters: \", model.model_params) \n",
    "    # print(\"Cross_validation report comparison: \")\n",
    "    # print(model.cv_report) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Embedding import Embedding, utils_preprocess_text\n",
    "from Model import Model, MODELS_PATH \n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable \n",
    "\n",
    "# taken from tutorial: https://closeheat.com/blog/pytorch-lstm-text-generation-tutorial\n",
    "from sklearn.datasets import make_classification\n",
    "from torch import nn\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken = X.shape[1], num_units = 128, \n",
    "                 nhid = 128, nlayers = 3, dropout=0.5, tie_weights=False):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, num_units)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(num_units, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(num_units, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != num_units:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
    "                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n",
    "        else:\n",
    "            return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conan_path = 'CONAN-master/Multitarget-CONAN/'\n",
    "conan_file = 'Multitarget-CONAN.json'\n",
    "conan_p = conan_path + conan_file\n",
    "\n",
    "davidson_path = 'hate-speech-and-offensive-language-master/data/'\n",
    "davidson_f = 'labeled_data.csv' \n",
    "davidson_p = davidson_path + davidson_f\n",
    "\n",
    "combined_path = ''\n",
    "combined_f = 'combined_dataset.csv'\n",
    "combined_p = combined_path + combined_f\n",
    "\n",
    "reports_path = 'classification_reports/'\n",
    "data_path = 'datasets/'\n",
    "model_path = 'models/'\n",
    "embed_path = 'embeds/'\n",
    "\n",
    "# command to activate virtual environment on Windows \n",
    "# cd into src folder \n",
    "# run Scripts\\activate.bat \n",
    "\n",
    "# list of classifiers to test \n",
    "classifiers = [\"Logistic Regression\", \"Random Forest\", \"Decision Trees\", \"XGBoost\", \"SVM\", \"Naive Bayes\"]\n",
    "\n",
    "# create lists of names for loading models \n",
    "class_list = [str.lower(clsf).replace(\" \", \"_\") + \".pkl\" for clsf in classifiers]# ['random_forest.pkl', 'svm.pkl']\n",
    "class_model_list =  [str.lower(clsf).replace(\" \", \"_\") + \"_model.pkl\" for clsf in classifiers] # ['random_forest_model.pkl, svm_model.pkl']\n",
    "# print(class_list)\n",
    "# print(class_model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Hate Speech Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier: # args are unnamed parameters (makes a list), and kwargs are named parameters (makes a dictionary)\n",
    "    def __init__(self, labels, *args, **kwargs): # X, y, **kwargs):   \n",
    "        self.labels = labels # enter a list of the class labels you want to output for the graphs\n",
    "        self.X = kwargs.get('X', None) # input independent variable data\n",
    "        self.y = kwargs.get('y', None) # target corresponding class variables with X\n",
    "        name_regex = '([a-zA-Z]*)(\\s*)([a-zA-Z]*)'\n",
    "        # determine generic classifier using this as a named parameter  \n",
    "        clsf = kwargs.pop('classifier', '') \n",
    "        normalize = lambda name: (name.group(1).strip().capitalize() + \" \" + name.group(3).strip().capitalize())\n",
    "        # formats all variations of name for classifer as 'Upper Case' \n",
    "        self.model_name = re.sub(name_regex, normalize, clsf).strip() # or re.sub(name_regex, normalize, kwargs.pop('model', ''))\n",
    "\n",
    "        # get model parameters (hyper parameters) as a named parameter \n",
    "        self.model_params = kwargs.get('model_params', None) # == 'model_params'\n",
    "        # selects normal model given model_name \n",
    "        if self.model_params is None:\n",
    "            self.model = LogisticRegression(random_state = 0, solver = 'lbfgs', warm_start = True) if self.model_name == \"Logistic Regression\" else \\\n",
    "                RandomForestClassifier(random_state = 0) if self.model_name == \"Random Forest\" else \\\n",
    "                GaussianNB() if self.model_name == \"Naive Bayes\" else \\\n",
    "                DecisionTreeClassifier(random_state = 0) if self.model_name == \"Decision Trees\" else \\\n",
    "                XGBClassifier(random_state = 0) if self.model_name == \"Xgboost\" else \\\n",
    "                SVC(random_state = 0, probability = True) # if self.model_name == \"SVM\" else None\n",
    "        else:\n",
    "            self.model = LogisticRegression(random_state = 0, **(self.model_params)) if self.model_name == \"Logistic Regression\" else \\\n",
    "                RandomForestClassifier(random_state = 0, **(self.model_params)) if self.model_name == \"Random Forest\" else \\\n",
    "                GaussianNB(**(self.model_params)) if self.model_name == \"Naive Bayes\" else \\\n",
    "                DecisionTreeClassifier(random_state = 0, **(self.model_params)) if self.model_name == \"Decision Trees\" else \\\n",
    "                XGBClassifier(random_state = 0, **(self.model_params)) if self.model_name == \"Xgboost\" else \\\n",
    "                SVC(random_state = 0, probability = True, **(self.model_params)) # if self.model_name == \"SVM\" else None\n",
    "        \n",
    "        # if passing model as named parameter \n",
    "        if self.model is None:\n",
    "            self.model = kwargs.get('model', None) # if self.model_name == \"SVM\" else None\n",
    "            \n",
    "        # get Cross Validator object as a named parameter \n",
    "        self.CV = kwargs.get('cv', None) # == 'cv'\n",
    "        print(\"cross validator\", self.CV)\n",
    "        # get cv parameter (parameters) \n",
    "        self.cv_params = kwargs.get('cv_params', None) # == 'cv_params'\n",
    "        # print(\"cross validator parameters\", self.cv_params)\n",
    "        # print(self.cv_params) \n",
    "        \n",
    "        # Retain model evaluation metrics\n",
    "        self.preds = None\n",
    "        self.cls_report = None \n",
    "        self.acc_report = None \n",
    "        self.train_score = None\n",
    "        self.test_score = None \n",
    "        self.conf_m = None  # confusion matrix for test data \n",
    "        # self.features_n = len(tfidf_vectorizer.vocabulary_)\n",
    "        self.filename = (self.model_name.lower().replace(\" \", \"_\"))  \n",
    "        self.old_model = None \n",
    "        if self.X is None: \n",
    "            self.X = args[0]\n",
    "        if self.y is None: \n",
    "            self.y = args[1] \n",
    "        if self.CV is not None and self.cv_params is not None: \n",
    "            self.CV, self.old_model, self.model = self.cross_validation(*args, **kwargs) \n",
    "            # model = self.model, model_params = self.model_params, \n",
    "                              # cv = self.CV, cv_params = self.cv_params)\n",
    "      \n",
    "    # Defines cross_validation function to test modified models\n",
    "    def cross_validation(self, *args, **kwargs): \n",
    "        model = self.model if self.model is not None else kwargs.pop('model', None)\n",
    "        model_params = self.model_params if self.model_params is not None else kwargs.pop('model_params', None)\n",
    "        cv = self.CV if self.CV is not None else kwargs.pop('cv', None)\n",
    "        cv_params = self.cv_params if self.cv_params is not None else kwargs.pop('cv_params', None)\n",
    "        debug = kwargs.pop('debug', False) \n",
    "        \n",
    "        cv_args = []\n",
    "        cv_kwargs = cv_params\n",
    "            \n",
    "        if debug: \n",
    "            print(\"cross_validation\")\n",
    "            print(\"model: \", self.model_name)\n",
    "            print(\"args\", args)\n",
    "            print(\"kwargs:\", kwargs)\n",
    "            print(\"model params:\", model_params) \n",
    "            print(\"cv params:\", cv_params)\n",
    "            print(\"param_grid:\", param_grid)\n",
    "            print(\"cv_args\", cv_args)\n",
    "            print(\"cv_kwargs\", cv_kwargs)\n",
    "        \n",
    "        self.CV = cv(model, **cv_kwargs) # *args, **default_args) # create cross validator model \n",
    "        if debug: \n",
    "            print(model)\n",
    "            print(self.CV) \n",
    "        self.CV.fit(self.X, self.y)\n",
    "        self.model = self.CV.best_estimator_  # replace old model with new best model \n",
    "        return self.CV, model, self.model \n",
    "    \n",
    "    def set_params(self, *args, **kwargs):\n",
    "        self.labels = kwargs.pop('labels', '') # should be labels \n",
    "        \n",
    "        name_regex = '([a-zA-Z]*)(\\s*)([a-zA-Z]*)'\n",
    "        clsf = kwargs.pop('classifier', '')\n",
    "        normalize = lambda name: (name.group(1).strip().capitalize() + \" \" + name.group(3).strip().capitalize())\n",
    "        self.model_name = re.sub(name_regex, normalize, clsf).strip() # or re.sub(name_regex, normalize, kwargs.pop('model', ''))\n",
    "        self.model = LogisticRegression(random_state = 0) if self.model_name == \"Logistic Regression\" else \\\n",
    "            RandomForestClassifier(random_state = 0) if self.model_name == \"Random Forest\" else \\\n",
    "            GaussianNB() if self.model_name == \"Naive Bayes\" else \\\n",
    "            DecisionTreeClassifier(random_state = 0) if self.model_name == \"Decision Trees\" else \\\n",
    "            XGBClassifier(random_state = 0) if self.model_name == \"Xgboost\" else \\\n",
    "            SVC(random_state = 0, probability = True) \n",
    "        if self.model is None:\n",
    "            self.model = kwargs.pop('model', None) # if self.model_name == \"SVM\" else None\n",
    "        self.model_params = kwargs.pop('model_params', None) # == 'model_params'\n",
    "        self.CV = kwargs.pop('cv', None) # == 'cv'\n",
    "        self.cv_params = kwargs.pop('cv_params', None) # == 'cv_params'\n",
    "        \n",
    "        self.preds = kwargs.pop('preds', None)\n",
    "        self.score = kwargs.pop('score', None)\n",
    "        self.cls_report = kwargs.pop('cls_report', None) \n",
    "        self.acc_report = kwargs.pop('acc_report', None)\n",
    "        self.conf_m = None \n",
    "        self.filename = (self.model_name.lower().replace(\" \", \"_\")) \n",
    "        \n",
    "    # Use this function with a model object and set parameters for that model from its scikit-learn documentation     \n",
    "    def set_model_params(self, *args, **kwargs): \n",
    "        self.model = LogisticRegression(args, kwargs, random_state = 0) if (self.model_name == \"Logistic Regression\") else \\\n",
    "            RandomForestClassifier(args, kwargs, random_state = 0) if (self.model_name == \"Random Forest\") else \\\n",
    "            GaussianNB(args, kwargs) if self.model_name == \"Naive Bayes\" else \\\n",
    "            DecisionTreeClassifier(args, kwargs, random_state = 0) if self.model_name == \"Decision Trees\" else \\\n",
    "            XGBClassifier(args, kwargs, random_state = 0) if self.model_name == \"Xgboost\" else \\\n",
    "            SVC(args, kwargs, random_state = 0) # if self.model_name == \"SVM\" else None\n",
    "        if self.model is None:\n",
    "            self.model = (kwargs.pop('model', None))(args, kwargs) # if self.model_name == \"SVM\" else None\n",
    "  \n",
    "    def get_model(self): # return current model instance \n",
    "        return self.model\n",
    "    \n",
    "    def load(self, infile = \"\", path = \"\"):  # load Classifier object and model from pkl\n",
    "        if infile is \"\": \n",
    "            with open(path + self.filename + \".pkl\", \"rb\") as file: # read byte \n",
    "                self = pickle.load(file)\n",
    "        else: \n",
    "            with open(infile, \"rb\") as file: # read byte \n",
    "                self = pickle.load(file)\n",
    "        return self # Classifier(labels, model) \n",
    "    \n",
    "    def load_model(self, infile = \"\", path = \"\"): \n",
    "        if infile is \"\": \n",
    "            with open(path + self.filename + \"_model.pkl\", \"rb\") as file: # read byte \n",
    "                self.model = pickle.load(file)\n",
    "        else: \n",
    "            with open(infile, \"rb\") as file: # read byte \n",
    "                self.model = pickle.load(file)\n",
    "        return self.model # Classifier(labels, model) \n",
    "    \n",
    "    def save(self, path = \"\", outfile = \"\"):\n",
    "        if outfile is \"\": \n",
    "            with open(path + self.filename + \".pkl\", \"wb\") as file: # write byte \n",
    "                pickle.dump(self, file)\n",
    "        else: \n",
    "            with open(outfile, \"wb\") as file: # write byte \n",
    "                pickle.dump(self, file)\n",
    "        return self \n",
    "    \n",
    "    def save_model(self, path = \"\", outfile = \"\"):\n",
    "        if outfile is \"\": \n",
    "            with open(path + self.filename + \"_model.pkl\", \"wb\") as file: # write byte \n",
    "                pickle.dump(self.model, file)\n",
    "        else: \n",
    "            with open(outfile, \"wb\") as file: # write byte \n",
    "                pickle.dump(self.model, file)\n",
    "        return self.model \n",
    "        \n",
    "    def fit(self, X_train, y_train):  # returns model object as well \n",
    "        self.model.fit(X_train, y_train)\n",
    "        return self.model \n",
    "        \n",
    "    def predict(self, X_test): \n",
    "        self.preds = self.model.predict(X_test)\n",
    "        return self.preds\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        self.preds_proba = self.model.predict_proba(X_test)\n",
    "        return self.preds_proba\n",
    "    \n",
    "    def score(self, X_test, y_test): \n",
    "        self.score = self.model.score(X_test, y_test) \n",
    "        return self.score\n",
    "    \n",
    "    def report_results(self, y_test = None, preds = None, output = True, save = False, path = \"\", outfile = \"\"): \n",
    "        cls_report = self.cls_report if (y_test is None) and (preds is None) else self.classification_report(y_test, preds, output = output) \n",
    "        acc_report = self.acc_report if (y_test is None) and (preds is None) else self.accuracy_score(y_test, preds, output = output)\n",
    "        conf_m = np.array_str(self.conf_m, precision = 3) if (y_test is None) and (preds is None) \\\n",
    "        else np.array_str(self.confusion_matrix(y_test, preds, output = output), precision = 3) \n",
    "    \n",
    "        report = cls_report + \"\\n\" + acc_report + \"\\n\" + conf_m\n",
    "\n",
    "        if save: \n",
    "            if outfile is \"\": \n",
    "                # report.to_csv(path + self.filename + \"_results.csv\", sep = \",\", index = False, encoding = 'utf8')\n",
    "                with open(path + self.filename + \"_results.txt\", \"w+\", encoding = \"utf8\") as file: # write byte \n",
    "                    file.write(report)\n",
    "            else: \n",
    "                # report.to_csv(outfile, sep = \",\", index = False, encoding = 'utf8')\n",
    "                with open(outfile, \"w+\", encoding = \"utf8\") as file: # write byte \n",
    "                    file.write(report)\n",
    "    \n",
    "    def classification_report(self, y_test, preds, output = True, save = False, path = \"\", outfile = \"\"): \n",
    "        self.cls_report = classification_report(y_test, preds)\n",
    "        if output: \n",
    "            print(self.cls_report)\n",
    "        if save: \n",
    "            report = classification_report(y_test, preds, output_dict = save)\n",
    "            report = pd.DataFrame(report).transpose()\n",
    "            if outfile is \"\": \n",
    "                report.to_csv(path + self.filename + \"_clsf_report.csv\", sep = \",\", index = False, encoding = 'utf8')\n",
    "                # with open(path + self.filename + \".csv\", encoding = \"utf8\") as file: # write byte \n",
    "                    # file.write(report)\n",
    "            else: \n",
    "                report.to_csv(outfile, sep = \",\", index = False, encoding = 'utf8')\n",
    "                # with open(outfile, encoding = \"utf8\") as file: # write byte \n",
    "                    # file.write(report) \n",
    "        return self.cls_report \n",
    "            \n",
    "    def accuracy_score(self, y_test, preds, output = True, save = False, path = \"\", outfile = \"\"): \n",
    "        self.train_score = self.model.score(X_train, y_train)\n",
    "        self.test_score = accuracy_score(y_test, preds)\n",
    "        report = '{} Train accuracy {:.3f}%'.format(self.model_name, self.train_score * 100) + '\\n' \\\n",
    "            + '{} Test accuracy {:.3f}%'.format(self.model_name, self.test_score * 100) + '\\n'\n",
    "        self.acc_report = report \n",
    "        if output: \n",
    "            print(report) \n",
    "        if save: \n",
    "            if outfile is \"\": \n",
    "                # report.to_csv(path + self.filename + \"acc_report.csv\", sep = \",\", index = False, encoding = 'utf8')\n",
    "                with open(path + self.filename + \"_acc_report.txt\", \"w+\", encoding = \"utf8\") as file: # write byte \n",
    "                    file.write(report)\n",
    "            else: \n",
    "                # report.to_csv(outfile, sep = \",\", index = False, encoding = 'utf8')\n",
    "                with open(outfile, \"w+\", encoding = \"utf8\") as file: # write byte \n",
    "                    file.write(report) \n",
    "        return self.acc_report   \n",
    "        \n",
    "    def confusion_matrix(self, y_test, preds, output = True, save = False, path = \"\", outfile = \"\"): \n",
    "        self.conf_m = confusion_matrix(y_test, preds)\n",
    "        if output:  \n",
    "            print('Confusion matrix: ')\n",
    "            print(self.conf_m)\n",
    "        \n",
    "        if save: \n",
    "            if outfile is \"\": \n",
    "                # report.to_csv(path + self.filename + \"conf_matrix.csv\", sep = \",\", index = False, encoding = 'utf8')\n",
    "                with open(path + self.filename + \"_conf_matrix.txt\", \"w+\", encoding = \"utf8\") as file: # write byte \n",
    "                    file.write(str(self.conf_m))\n",
    "            else: \n",
    "                # report.to_csv(outfile, sep = \",\", index = False, encoding = 'utf8')\n",
    "                with open(outfile, \"w+\", encoding = \"utf8\") as file: # write byte \n",
    "                    file.write(str(self.conf_m)) \n",
    "        return self.conf_m\n",
    "    \n",
    "    def heatmap(self, conf_m = None, labels = None, save = False, num = False, path = \"\", outfile = \"\", model_num = \"\"):  # draw confusion matrix \n",
    "        conf_m = self.conf_m if conf_m is None else conf_m \n",
    "        labels = self.labels if labels is None else labels \n",
    "        \n",
    "        size = len(conf_m)\n",
    "        matrix = np.zeros((size, size))\n",
    "        for i in range(0, size):\n",
    "            matrix[i, :] = (conf_m[i, :])/(float(conf_m[i,:].sum())) # calculate percentage matrix of true positives\n",
    "            \n",
    "        conf_df = pd.DataFrame(matrix, index = labels, columns = labels)\n",
    "        plt.figure(figsize=(size * 1.5, size * 1.25))\n",
    "        \n",
    "        sns.heatmap(conf_df, annot = True, annot_kws = {\"size\": size * 1.33},\n",
    "                        cmap = \"YlGnBu\", # 'gist_gray_r', \n",
    "                    cbar = False, square = True, fmt = '.2f')\n",
    "        \n",
    "        plt.ylabel('True categories', fontsize = size * 1.5)\n",
    "        plt.xlabel('Predicted categories', fontsize = size * 1.5)\n",
    "        plt.tick_params(labelsize = size * 1.33)\n",
    "        \n",
    "        if save:\n",
    "            f_name = self.filename + \"_heatmap\" # \"_\".join(self.model_name.lower())\n",
    "            \n",
    "            if outfile is \"\":\n",
    "                plt.savefig(path + f_name + \".pdf\") # '.pdf')\n",
    "            else: \n",
    "                plt.savefig(outfile)\n",
    "        \n",
    "        # second heatmap with category numbers rather than percentile \n",
    "        if num: \n",
    "            conf_df = pd.DataFrame(conf_m, index = labels, columns = labels)\n",
    "            plt.figure(figsize = (size * 1.5, size * 1.25))\n",
    "            sns.heatmap(conf_df, annot = True, annot_kws = {\"size\": size * 1.33},\n",
    "                        cmap = \"YlGnBu\", # 'gist_gray_r', \n",
    "                    cbar = False, square = True, fmt = '5d')\n",
    "            \n",
    "            plt.ylabel('True categories', fontsize = size * 1.5)\n",
    "            plt.xlabel('Predicted categories', fontsize = size * 1.5)\n",
    "            plt.tick_params(labelsize = size * 1.33)\n",
    "            if save:\n",
    "                f_name = self.filename + \"_heatmap_num\" # \"_\".join(self.model_name.lower())\n",
    "            \n",
    "                if outfile is \"\":\n",
    "                    plt.savefig(path + f_name + str(model_num) +  \".pdf\") # + \"_num.pdf\") # '.pdf')\n",
    "                else:\n",
    "                    print(outfile[:-5] + str(model_num) + \".pdf\")\n",
    "                    plt.savefig(outfile[:-5] + str(model_num) + \".pdf\") # '.pdf')\n",
    "\n",
    "                \n",
    "    def roc_curve(self, X_test, y_test, save = False, path = \"\", outfile = \"\"):\n",
    "        size = len(self.labels) # compare to number of classes to format and size output \n",
    "        y_probs = self.model.predict_proba(X_test)\n",
    "        skplt.metrics.plot_roc(y_test, y_probs, figsize = (size * 1.5, size * 1.25), title_fontsize = size * 1.75, \n",
    "                               text_fontsize = size * 1.5)\n",
    "           \n",
    "        # plt.show()\n",
    "        # plt.ylabel('True positive rate', fontsize = size * 1.5)\n",
    "        # plt.xlabel('False positive rate', fontsize = size * 1.5)\n",
    "        # plt.tick_params(labelsize = size * 1.33)\n",
    "        \n",
    "        if save:\n",
    "            f_name = self.filename + \"_roc_curves\" # \"_\".join(self.model_name.lower())\n",
    "            if outfile is \"\":\n",
    "                plt.savefig(path + f_name + \".pdf\") # '.pdf')\n",
    "            else: \n",
    "                plt.savefig(outfile)\n",
    "        \n",
    "# Report classifications to file  \n",
    "# Be able to allow Lesh and Kimberly to train  and get a predictive value (and also true, false positive rates \n",
    "# and recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + \"combined_data.csv\", \"r\", encoding = \"utf8\") as file: \n",
    "    combined_hate_df = pd.read_csv(file)\n",
    "    file.close() \n",
    "    \n",
    "with open(data_path + \"combined_class.csv\", \"r\", encoding = \"utf8\") as file: \n",
    "    combined_tar_df = pd.read_csv(file)\n",
    "    file.close() \n",
    "    \n",
    "# with open(data_path + \"combined_labels.csv\", \"r\", encoding = \"utf8\") as file: \n",
    "#     label_names = pd.read_csv(file) \n",
    "#     file.close()\n",
    "    \n",
    "# display(combined_hate_df)\n",
    "# display(combined_tar_df)\n",
    "combined_hate_list = combined_hate_df.iloc[:, 0] \n",
    "# display(combined_hate_list) \n",
    "# combined_hate_df \n",
    "combined_tar_list = combined_tar_df.iloc[:, 0]\n",
    "# display(combined_tar_list)\n",
    "# label_names = list(label_names)\n",
    "label_names = ['Disabled', 'Jews', 'LGBT+', 'Migrants', 'Muslims', 'POC', 'Women', 'Other/Mixed', 'None']\n",
    "# labels = label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_tar_orig = combined_tar_list\n",
    "# labels = np.unique(min_tar_orig)\n",
    "\n",
    "# display(labels)\n",
    "display(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tfidf vectorizer embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Features-F1\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "# max and min are cutoffs for document frequency \n",
    "\n",
    "# Word Embeddings and Feature Selection \n",
    "# tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
    "#                                    max_df = 0.75, min_df = 5, # 0.7 - 1.0 for max_df takes care of stop words\n",
    "#                                    max_features = 10000)\n",
    "# # TF-IDF feature matrix\n",
    "# docs = tfidf_vectorizer.fit_transform(combined_hate_list)\n",
    "# features = tfidf_vectorizer.get_feature_names() # _out(input_features = None)\n",
    "# print(len(features))\n",
    "\n",
    "# Not necessary with max_df \n",
    "# stop_words = tfidf_vectorizer.get_stop_words()\n",
    "# print(stop_words)\n",
    "\n",
    "# embed_params = tfidf_vectorizer.get_params()\n",
    "\n",
    "# with open(embed_path + \"tfidf_vectorizer.pkl\", \"wb\") as file: # write byte \n",
    "#     pickle.dump(save_tfidf_vectorizer, file)\n",
    "#     file.close() \n",
    "  \n",
    "# print(embed_params)\n",
    "# display(docs)\n",
    "# display(tfidf_vectorizer.vocabulary_)\n",
    "# display(tfidf_vectorizer.idf_)\n",
    "\n",
    "# encoded vector\n",
    "# display(docs.shape)\n",
    "# display(docs.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tfidf vectorizer embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embed_path + \"tfidf_vectorizer.pkl\", \"rb\") as file: # write byte \n",
    "    tfidf_vectorizer = pickle.load(file)\n",
    "    file.close() \n",
    "    \n",
    "# TF-IDF feature matrix\n",
    "docs = tfidf_vectorizer.fit_transform(combined_hate_list)\n",
    "features = tfidf_vectorizer.get_feature_names() # _out(input_features = None)\n",
    "print(len(features))\n",
    "\n",
    "# Not necessary with max_df \n",
    "# stop_words = tfidf_vectorizer.get_stop_words()\n",
    "# print(stop_words)\n",
    "embed_params = tfidf_vectorizer.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit, predict, and hyperparameter tune models with Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runs Logistic Regression, Random Forest, Decision Trees, XGBoost, SVM, and Naive Bayes algorithms in that order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers = [\"Logistic Regression\", \"Random Forest\", \"Decision Trees\", \"XGBoost\", \"SVM\", \"Naive Bayes\"]\n",
    "# reports_path = 'classification_reports/'\n",
    "# data_path = 'datasets/'\n",
    "# model_path = 'models/'\n",
    "# embed_path = 'embeds/' \n",
    "\n",
    "\n",
    "# def confusion_matrix_scorer(clf, X_train, y_train):\n",
    "#     y_preds = clf.predict(X_train)\n",
    "#     conf_m = confusion_matrix(y_train, y_preds)\n",
    "#     return {'true negative': conf_m[0, 0], 'false positive': conf_m[0, 1],\n",
    "#             'false negative': conf_m[1, 0], 'true positive': conf_m[1, 1]}\n",
    "\n",
    "model_num = 0 \n",
    "\n",
    "output = True \n",
    "save = True # False \n",
    "num = True\n",
    "\n",
    "cross_validate = False # whether to fine tune the hyper parameters of the model with a cross validator object \n",
    "debug = False\n",
    "outfile = \"\"\n",
    "\n",
    "\n",
    "cv = None \n",
    "X = docs\n",
    "y = combined_tar_list\n",
    "\n",
    "for i in range((len(classifiers))): \n",
    "    if cross_validate: \n",
    "        cv = GridSearchCV # almost always uses StratifiedKFold with 5 splits \n",
    "        cv_params = None # [[{'random_state' : 0}]]\n",
    "        \n",
    "    if (classifiers[i] == \"Logistic Regression\"): \n",
    "        model_params = None\n",
    "        if cross_validate: \n",
    "            model_params = {'max_iter': 1000 # control number of iterations for regression convergence \n",
    "                           }\n",
    "            solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "            penalty = ['l2']\n",
    "            C = [100, 10, 1.0, 0.1, 0.01]\n",
    "            # define grid search\n",
    "            param_grid = {\n",
    "                          'penalty': penalty,\n",
    "                          'C': c_values,\n",
    "                          'solver': solvers, # penalty, C, solver is order \n",
    "                         }\n",
    "            \n",
    "            cv_params = {'param_grid': param_grid, \n",
    "                         # 'scorer': confusion_matrix_scorer, \n",
    "                         'verbose': 3\n",
    "                        } # [param_grid, {'verbose': 3}]\n",
    "    elif (classifiers[i] == \"Random Forest\"):  # do with randomized search \n",
    "        model_params = None\n",
    "        if cross_validate: \n",
    "            cv = RandomizedSearchCV # almost always uses StratifiedKFold with 5 splits \n",
    "            # Number of trees in random forest\n",
    "            n_estimators = [int(x) for x in np.linspace(start = 800, stop = 2000, num = 3)] # [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "            # Number of features to consider at every split\n",
    "            max_features = ['auto', 'sqrt']\n",
    "            # Maximum number of levels in tree\n",
    "            max_depth = [int(x) for x in np.linspace(10, 110, num = 3)] # [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "            max_depth.append(None)\n",
    "            # Minimum number of samples required to split a node\n",
    "            min_samples_split = [2, 5, 10]\n",
    "            # Minimum number of samples required at each leaf node\n",
    "            min_samples_leaf = [1, 2, 4]\n",
    "            bootstrap = [True, False]\n",
    "            # param_grid = {'base_estimator__max_depth': [2, 4, 6, 8]}\n",
    "            model_params = None\n",
    "            # Create the param grid\n",
    "            param_grid = { \n",
    "                           'n_estimators': n_estimators,\n",
    "                           'max_features': max_features,\n",
    "                           'max_depth': max_depth,\n",
    "                           'min_samples_split': min_samples_split,\n",
    "                           'min_samples_leaf': min_samples_leaf,\n",
    "                           'bootstrap': bootstrap # Method of selecting samples for training each tree\n",
    "                         }\n",
    "            cv_params = {'param_distributions': param_grid, \n",
    "                         # 'scorer': confusion_matrix_scorer, \n",
    "                         'verbose': 2\n",
    "                        } # [param_grid, {'verbose': 3}]\n",
    "    elif (classifiers[i] == \"Decision Trees\"): \n",
    "        model_params = None\n",
    "        if cross_validate: \n",
    "            # choose function to measure quality of node split \n",
    "            criterion = [\"gini\", \"entropy\"]\n",
    "            model_params = None\n",
    "            param_grid = {'criterion' : criterion,\n",
    "                          'max_features': max_features,\n",
    "                          'max_depth': max_depth,\n",
    "                          'min_samples_split': min_samples_split,\n",
    "                          'min_samples_leaf': min_samples_leaf,\n",
    "                         }\n",
    "            cv_params = {'param_grid': param_grid, \n",
    "                         # 'scorer': confusion_matrix_scorer, \n",
    "                         'verbose': 2\n",
    "                        } # [param_grid, {'verbose': 3}]\n",
    "    elif (classifiers[i] == \"XGBoost\"): \n",
    "        model_params = None\n",
    "        if cross_validate: \n",
    "            cv = RandomizedSearchCV\n",
    "            param_grid = {'objective': ['reg:squarederror', 'reg:squaredlogerror'],\n",
    "                          'max_depth': [3, 6, 10],\n",
    "                          'learning_rate': [0.01, 0.05, 0.1],\n",
    "                          'n_estimators': [100, 500, 1000],\n",
    "                          'colsample_bytree': [0.3, 0.7]\n",
    "                          }        \n",
    "            cv_params = {'param_distributions': param_grid, \n",
    "                         # 'scorer': confusion_matrix_scorer, \n",
    "                         'verbose': 2\n",
    "                        } # [param_grid, {'verbose': 3}]\n",
    "    elif (classifiers[i] == \"SVM\"):  # Support Vector Machine\n",
    "        model_params = None\n",
    "        if cross_validate: \n",
    "            cv = GridSearchCV # RandomizedSearchCV\n",
    "            param_grid = { \n",
    "                           'C': [0.1, 1, 10, 100], \n",
    "                           'gamma': [1 , 0.1, 0.01, 0.001]\n",
    "                         } \n",
    "            \n",
    "            cv_params = {'param_grid': param_grid, \n",
    "                         # 'scorer': confusion_matrix_scorer, \n",
    "                         # 'refit': True, \n",
    "                         'probability': True, \n",
    "                         'verbose': 2\n",
    "                        } # [param_grid, {'verbose': 3}]\n",
    "    elif (classifiers[i] == \"Naive Bayes\"):\n",
    "        X = X.toarray()\n",
    "        model_params = None\n",
    "        if cross_validate: \n",
    "            param_grid = {'var_smoothing': np.logspace(0, -9, num = 100)\n",
    "        # np.<a onclick=\"parent.postMessage({'referent':'.numpy.logspace'}, '*')\">logspace(0,-9, num=100)}\n",
    "                         }     \n",
    "            cv_params = {'param_grid': param_grid, \n",
    "                         # 'scorer': confusion_matrix_scorer, \n",
    "                         'verbose': 3\n",
    "                        } # [param_grid, {'verbose': 3}]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.2) \n",
    "    \n",
    "    if cross_validate:\n",
    "        model = Classifier(labels, X_train, y_train, debug = debug, classifier = classifiers[i], model_params = model_params,\n",
    "                           cv = cv, cv_params = cv_params) \n",
    "    else: \n",
    "        model = Classifier(labels, X_train, y_train, debug = debug, classifier = classifiers[i], model_params = model_params) \n",
    "\n",
    "    # Example results \n",
    "    cv = model.CV\n",
    "    if outfile is \"\" and cross_validate: \n",
    "        filename = model.filename\n",
    "        # report.to_csv(path + self.filename + \"conf_matrix.csv\", sep = \",\", index = False, encoding = 'utf8')\n",
    "        cv_path = filename + \"_cross_validation_scoring.txt\" # \"_cross_validation_scoring_\" + str(model_num) + \".pdf\"\n",
    "        with open(reports_path + cv_path, \"w+\", encoding = \"utf8\") as file: # write byte \n",
    "            file.write(classifiers[i] + \" Best: %f using %s\\n\" % (cv.best_score_, cv.best_params_))\n",
    "            # print(classifiers[i], \"Best: %f using %s\" % (cv.best_score_, cv.best_params_))\n",
    "            means = cv.cv_results_['mean_test_score']\n",
    "            stds = cv.cv_results_['std_test_score']\n",
    "            params = cv.cv_results_['params']\n",
    "            for mean, stdev, param in zip(means, stds, params):\n",
    "                file.write(\"%f (%f) with: %r\\n\" % (mean, stdev, param))\n",
    "                # print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    elif outfile is not \"\": \n",
    "        # report.to_csv(outfile, sep = \",\", index = False, encoding = 'utf8')\n",
    "        with open(outfile, \"w+\", encoding = \"utf8\") as file: # write byte \n",
    "            file.write(classifiers[i], \"Best: %f using %s\\n\" % (cv.best_score_, cv.best_params_))\n",
    "            # print(classifiers[i], \"Best: %f using %s\" % (cv.best_score_, cv.best_params_))\n",
    "            means = cv.cv_results_['mean_test_score']\n",
    "            stds = cv.cv_results_['std_test_score']\n",
    "            params = cv.cv_results_['params']\n",
    "            for mean, stdev, param in zip(means, stds, params):\n",
    "                file.write(\"%f (%f) with: %r\\n\" % (mean, stdev, param))\n",
    "                # print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_test)\n",
    "    # y_probs = model.predict_proba(X_test)\n",
    "    \n",
    "    old_path = (classifiers[i].lower().replace(\" \", \"_\")) + \"_clsf_report_\" + str(model_num) + \".txt\"\n",
    "    old_heatmap_path = (classifiers[i].lower().replace(\" \", \"_\")) + \"_heat_map_\" + str(model_num) + \".pdf\"\n",
    "    old_roc_path = (classifiers[i].lower().replace(\" \", \"_\")) + \"_roc_curves_\" + str(model_num) + \".pdf\"\n",
    "    # print(old_path)\n",
    "    # print(old_heatmap_path)\n",
    "    # print(old_roc_path)\n",
    "    cls_report = model.classification_report(y_test, y_preds, output = output, save = save, \n",
    "                                             outfile = reports_path + old_path) #, path = reports_path)\n",
    "    \n",
    "    old_path = (classifiers[i].lower().replace(\" \", \"_\")) + \"_acc_report_\" + str(model_num) + \".txt\"\n",
    "    acc_report = model.accuracy_score(y_test, y_preds, outfile = reports_path + old_path)\n",
    "    \n",
    "    old_path = (classifiers[i].lower().replace(\" \", \"_\")) + \"_conf_matrix_\" + str(model_num) + \".txt\"\n",
    "    conf_m = model.confusion_matrix(y_test, y_preds, output = output, save = save, \n",
    "                                    outfile = reports_path + old_path) #, path = reports_path)\n",
    "\n",
    "    model.heatmap(save = save, num = num, outfile = reports_path + old_heatmap_path) # path = reports_path)\n",
    "    model.roc_curve(X_test, y_test, save = save, outfile = reports_path + old_roc_path) # path = reports_path)\n",
    "    model.report_results(output = output, save = save, outfile = reports_path + old_path) # path = reports_path) \n",
    "    \n",
    "    model.save_model(path = model_path)\n",
    "    model.save(path = model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = docs\n",
    "y = combined_tar_list\n",
    "choice = 0 \n",
    "\n",
    "save = False\n",
    "\n",
    "'''Load last updated classifier object''' \n",
    "new_clsf = Classifier(labels).load(infile = model_path + class_list[choice])\n",
    "print(new_clsf.get_model()) \n",
    "#OR \n",
    "\n",
    "'''Create new classifier object and pass it copy of last updated model''' \n",
    "# sec_clsf = Classifier(labels) # labels, classifier = classifiers[1])                                                              \n",
    "# load_model = sec_clsf.load_model(infile = model_path + class_model_list[choice])\n",
    "# print(load_model)\n",
    "\n",
    "'''Two ways to load up and test models'''\n",
    "y_preds = new_clsf.predict(X_test)\n",
    "print(y_preds)\n",
    "\n",
    "# sec_y_preds = sec_clsf.predict(X_test)\n",
    "# print(sec_y_preds)\n",
    "\n",
    "''' test model only ''' \n",
    "# md_y_preds = load_model.predict(X_test)\n",
    "# print(md_y_preds)\n",
    "\n",
    "# # new_clsf.set_params(model = load_model)\n",
    "\n",
    "# # sec_y_preds.set_params(labels = labels, classifier = classifiers[1])\n",
    "\n",
    "# cls_report = new_clsf.classification_report(y_test, y_preds)\n",
    "acc_report = new_clsf.accuracy_score(y_test, y_preds)\n",
    "print(new_clsf.model_name)\n",
    "# conf_m = new_clsf.confusion_matrix(y_test, y_preds)\n",
    "\n",
    "# new_clsf.report_results(y_test, y_preds)\n",
    "# new_clsf.heatmap()\n",
    "\n",
    "# cls_report = sec_clsf.classification_report(y_test, y_preds)\n",
    "acc_report = sec_clsf.accuracy_score(y_test, y_preds) \n",
    "print(sec_clsf.model_name) # this is empty for sec_model\n",
    "\n",
    "# conf_m = sec_clsf.confusion_matrix(y_test, y_preds)\n",
    "\n",
    "# sec_clsf.report_results(y_test, y_preds)\n",
    "# sec_clsf.heatmap()\n",
    "\n",
    "# # look at set_model_params() with\n",
    "# load_model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # probability for the label, and the label itself (parallel list) \n",
    "save = False \n",
    "output = True \n",
    "\n",
    "X = docs\n",
    "y = combined_tar_list \n",
    "# label_names are types of 9 targets \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.2)\n",
    "\n",
    "# print(X.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "test_model_0 = Classifier(label_names, classifier = classifiers[0]) \n",
    "\n",
    "test_model_0.fit(X_train, y_train)\n",
    "y_preds = test_model_0.predict(X_test)\n",
    "y_probs = test_model_0.predict_proba(X_test)\n",
    "\n",
    "# display(X_test)\n",
    "# display(y_test)\n",
    "display(pd.concat([pd.DataFrame(y_preds, columns = [\"prediction\"]), pd.DataFrame(y_probs, columns = label_names)], axis = 1))\n",
    "\n",
    "cls_report = test_model_0.classification_report(y_test, y_preds, output = output, save = save, path = reports_path)\n",
    "acc_report = test_model_0.accuracy_score(y_test, y_preds)\n",
    "conf_m = test_model_0.confusion_matrix(y_test, y_preds, output = output, save = save, path = reports_path)\n",
    "\n",
    "test_model_0.heatmap(save = save, path = reports_path)\n",
    "test_model_0.report_results(output = output, save = save, path = reports_path) \n",
    "\n",
    "# test_model_0.save_model(path = model_path)\n",
    "# test_model_0.save(path = model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider n for self-created test set  n = 200 or n = 250 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LSTM and CNN *** sharfard paper \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "X = docs\n",
    "y = combined_tar_list\n",
    "\n",
    "# min_max = MinMaxScaler()\n",
    "# standard_scale = StandardScaler()\n",
    "# tfidf_X = tfidf_vectorizer.transform(X).toarray()\n",
    "# tfidf_y = tfidf_vectorizer.transform(y).toarray()\n",
    "# print(len(tfidf_X))\n",
    "# print(len(tfidf_y))\n",
    "\n",
    "tfidf_X = X\n",
    "tfidf_y = y\n",
    "X_ss = tfidf_X # standard_scale.fit_transform(tfidf_X)\n",
    "y_mm = tfidf_y # min_max.fit_transform(tfidf_y) \n",
    "# display(X_ss)\n",
    "# display(y_mm)\n",
    "\n",
    "train_num = int(0.8 * len(y)) \n",
    "# print(train_num)\n",
    "# Must keep data in sequential order for memory learning \n",
    "X_train = X_ss[:train_num, :]\n",
    "X_test = X_ss[train_num:, :]\n",
    "# print(\"X_train \")\n",
    "# print(X_train)\n",
    "# print(\"X_test \")\n",
    "# print(X_test)\n",
    "\n",
    "# print(X_ss)\n",
    "# print(y_mm)\n",
    "y_train = y_mm[:train_num] # , :]\n",
    "y_test = y_mm[train_num:] # , :]\n",
    "print(\"y_train \", y_train)\n",
    "print(\"y_test \", y_test)\n",
    "\n",
    "# max_review_length = 500\n",
    "# X_train = sequence.pad_sequences(X_train, maxlen = max_review_length)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen = max_review_length)\n",
    "\n",
    "# print(\"Training Shape\", X_train.shape, y_train.shape)\n",
    "# print(\"Testing Shape\", X_test.shape, y_test.shape)\n",
    "\n",
    "from torch.autograd import Variable \n",
    "X_train_tensors = Variable(torch.Tensor(X_train))# .shape[0]))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))# .shape[0]))\n",
    "display(X_train_tensors)\n",
    "display(X_test_tensors)\n",
    "\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "display(y_train_tensors)\n",
    "y_test_tensors = Variable(torch.Tensor(list(y_test))) \n",
    "display(y_test_tensors)\n",
    "\n",
    "X_train_tensors_final = torch.reshape(X_train_tensors, \n",
    "                                      (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
    "X_test_tensors_final = torch.reshape(X_test_tensors,  \n",
    "                                     (X_test_tensors.shape[0], 1, X_test_tensors.shape[1])) \n",
    "display(X_train_tensors_final)\n",
    "display(X_test_tensors_final)\n",
    "\n",
    "display(y_train_tensors)\n",
    "display(y_test_tensors)\n",
    "\n",
    "# lstm = torch.nn.LSTM(X) # 10, 20, 2)\n",
    "# output, (hn, cn) = lstm(inputs, (h0, c0))\n",
    "\n",
    "# create the model\n",
    "embed_vec_length = 32\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(train_num, embed_vec_length))# , input_length = train_num))\n",
    "\n",
    "lstm.add(LSTM(100))\n",
    "\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropogate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(\n",
    "    torch.randn((1, 1, 3))))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)\n",
    "\n",
    "# lstm.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "lstm.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "lstm.summary()\n",
    "lstm.fit(X_train, y_train, batch_size = 32) # epochs = 3, batch_size = 64)\n",
    "\n",
    "# print(\"Output:\")\n",
    "# print(output)\n",
    "# print()\n",
    "# print(\"hn:\")\n",
    "# print(hn)\n",
    "# print()\n",
    "# print(\"cn:\")\n",
    "# print(cn)\n",
    "# lstm.compile(loss='cosine_proximity', optimizer='sgd', metrics = ['accuracy'])\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = lstm.evaluate(X_test, y_test, epochs = 1, verbose = 0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
    "nn_model.add(GRU(256, return_sequences=True))\n",
    "\n",
    "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
    "nn_model.add(SimpleRNN(128))\n",
    "\n",
    "nn_model.add(Dense(10))\n",
    "\n",
    "nn_model.summary()\n",
    "\n",
    "nn_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "nn_model.fit(X_train, y_train, epochs = 3, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considers params and gamma heavily with gaussian\n",
    "\n",
    "# model selection and hyperparamter tuning step \n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1 , 0.1, 0.01, 0.001]} \n",
    "svm_grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
    "\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "y_preds = svm_grid.predict(X_test)\n",
    "report = classification_report(y_test, y_preds)\n",
    "acc4 = accuracy_score(y_test, y_preds)\n",
    "mod_train4 = svm_grid.score(X_train, y_train) \n",
    "\n",
    "print(report)\n",
    "\n",
    "print('SVM Train accuracy {:.3f}%'.format(mod_train4 * 100)) \n",
    "print('SVM Test accuracy {:.3f}%'.format(acc4 * 100)) \n",
    "\n",
    "print('Confusion matrix: ')\n",
    "conf_m = confusion_matrix(y_test, y_preds)\n",
    "print(conf_m)\n",
    "make_heatmap(conf_m, labels, save = False, out_file = output_path + classifiers[0] + '_combined.pdf')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcn_venv",
   "language": "python",
   "name": "tcn_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
